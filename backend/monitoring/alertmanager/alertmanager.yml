# Alertmanager Configuration for Multilingual Mandi
# Requirements: 18.4

global:
  resolve_timeout: 5m
  # SMTP configuration for email alerts (configure with actual values)
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@multilingual-mandi.com'
  smtp_auth_username: 'alerts@multilingual-mandi.com'
  smtp_auth_password: 'CHANGE_ME'
  smtp_require_tls: true

# Templates for alert notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing
route:
  # Default receiver for all alerts
  receiver: 'default-receiver'
  
  # Group alerts by these labels
  group_by: ['alertname', 'cluster', 'service']
  
  # Wait time before sending initial notification
  group_wait: 10s
  
  # Wait time before sending notification about new alerts in group
  group_interval: 10s
  
  # Wait time before re-sending notification
  repeat_interval: 12h
  
  # Child routes for specific alert types
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 5s
      repeat_interval: 4h
      continue: true
    
    # Latency alerts - specific handling
    - match_re:
        alertname: '.*Latency'
      receiver: 'latency-alerts'
      group_wait: 30s
      repeat_interval: 6h
    
    # System health alerts
    - match:
        component: system
      receiver: 'system-alerts'
      repeat_interval: 8h
    
    # Accuracy/quality alerts
    - match:
        component: '(stt_service|translation_service)'
      receiver: 'quality-alerts'
      repeat_interval: 12h

# Receivers define notification destinations
receivers:
  # Default receiver - logs to console
  - name: 'default-receiver'
    webhook_configs:
      - url: 'http://backend:8000/api/v1/alerts/webhook'
        send_resolved: true

  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@multilingual-mandi.com'
        headers:
          Subject: '[CRITICAL] Multilingual Mandi Alert: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.default.html" . }}'
    webhook_configs:
      - url: 'http://backend:8000/api/v1/alerts/webhook'
        send_resolved: true
    # Add Slack/PagerDuty/etc. configurations here
    # slack_configs:
    #   - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    #     channel: '#alerts-critical'
    #     title: 'Critical Alert: {{ .GroupLabels.alertname }}'
    #     text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  # Latency alerts - performance team
  - name: 'latency-alerts'
    email_configs:
      - to: 'performance-team@multilingual-mandi.com'
        headers:
          Subject: '[LATENCY] Multilingual Mandi: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.default.html" . }}'
    webhook_configs:
      - url: 'http://backend:8000/api/v1/alerts/webhook'
        send_resolved: true

  # System alerts - infrastructure team
  - name: 'system-alerts'
    email_configs:
      - to: 'infrastructure@multilingual-mandi.com'
        headers:
          Subject: '[SYSTEM] Multilingual Mandi: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.default.html" . }}'
    webhook_configs:
      - url: 'http://backend:8000/api/v1/alerts/webhook'
        send_resolved: true

  # Quality alerts - ML/data team
  - name: 'quality-alerts'
    email_configs:
      - to: 'ml-team@multilingual-mandi.com'
        headers:
          Subject: '[QUALITY] Multilingual Mandi: {{ .GroupLabels.alertname }}'
        html: '{{ template "email.default.html" . }}'
    webhook_configs:
      - url: 'http://backend:8000/api/v1/alerts/webhook'
        send_resolved: true

# Inhibition rules - suppress certain alerts when others are firing
inhibit_rules:
  # Inhibit warning alerts if critical alert is firing for same component
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['component', 'instance']
  
  # Inhibit specific service alerts if service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*Latency|.*ErrorRate'
    equal: ['instance']
